{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9da01ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 29 00:44:32 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA H200                    On  |   00000000:53:00.0 Off |                    0 |\r\n",
      "| N/A   32C    P0            102W /  700W |       1MiB / 143771MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400ec4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/*\r\n"
     ]
    }
   ],
   "source": [
    "!echo ./datasets/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a35a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGELOG.md  dotah_generator\t    models\t\t      trainers\r\n",
      "cmd.lnk       etc\t\t    output_models\t      train.py\r\n",
      "config.py     generate_data.bat     __pycache__\t\t      train_rcan.bat\r\n",
      "configs       Instructions.docx     raw_data\t\t      utils\r\n",
      "data\t      KJRD_net_train.ipynb  ReadMe.md\r\n",
      "data_source   mae\t\t    requirements_windows.yml\r\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4363db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\r\n",
      "------------------------ -----------\r\n",
      "absl-py                  2.2.2\r\n",
      "accelerate               1.6.0\r\n",
      "aiohappyeyeballs         2.6.1\r\n",
      "aiohttp                  3.11.14\r\n",
      "aiosignal                1.3.2\r\n",
      "annotated-types          0.7.0\r\n",
      "asttokens                3.0.0\r\n",
      "async-timeout            5.0.1\r\n",
      "attrs                    25.3.0\r\n",
      "beautifulsoup4           4.13.4\r\n",
      "certifi                  2025.4.26\r\n",
      "charset-normalizer       3.4.1\r\n",
      "comm                     0.2.2\r\n",
      "contourpy                1.3.2\r\n",
      "cycler                   0.12.1\r\n",
      "dacite                   1.9.2\r\n",
      "datasets                 3.5.0\r\n",
      "debugpy                  1.8.14\r\n",
      "decorator                5.2.1\r\n",
      "dill                     0.3.8\r\n",
      "evaluate                 0.4.3\r\n",
      "exceptiongroup           1.2.2\r\n",
      "executing                2.2.0\r\n",
      "filelock                 3.18.0\r\n",
      "fonttools                4.57.0\r\n",
      "frozenlist               1.5.0\r\n",
      "fsspec                   2024.12.0\r\n",
      "gdown                    5.2.0\r\n",
      "grad-cam                 1.5.5\r\n",
      "h5py                     3.13.0\r\n",
      "htmlmin                  0.1.12\r\n",
      "huggingface-hub          0.30.2\r\n",
      "idna                     3.10\r\n",
      "ImageHash                4.3.1\r\n",
      "ipykernel                6.29.5\r\n",
      "ipython                  8.36.0\r\n",
      "ipywidgets               8.1.5\r\n",
      "jedi                     0.19.2\r\n",
      "Jinja2                   3.1.6\r\n",
      "joblib                   1.4.2\r\n",
      "jupyter_client           8.6.3\r\n",
      "jupyter_core             5.7.2\r\n",
      "jupyterlab_widgets       3.0.13\r\n",
      "keras                    3.7.0\r\n",
      "kiwisolver               1.4.8\r\n",
      "llvmlite                 0.44.0\r\n",
      "markdown-it-py           3.0.0\r\n",
      "MarkupSafe               3.0.2\r\n",
      "matplotlib               3.10.1\r\n",
      "matplotlib-inline        0.1.7\r\n",
      "mdurl                    0.1.2\r\n",
      "ml_dtypes                0.5.1\r\n",
      "mpmath                   1.3.0\r\n",
      "multidict                6.2.0\r\n",
      "multimethod              1.12\r\n",
      "multiprocess             0.70.16\r\n",
      "namex                    0.0.9\r\n",
      "nest-asyncio             1.6.0\r\n",
      "networkx                 3.4.2\r\n",
      "numba                    0.61.0\r\n",
      "numpy                    1.26.4\r\n",
      "nvidia-cublas-cu12       12.4.5.8\r\n",
      "nvidia-cuda-cupti-cu12   12.4.127\r\n",
      "nvidia-cuda-nvrtc-cu12   12.4.127\r\n",
      "nvidia-cuda-runtime-cu12 12.4.127\r\n",
      "nvidia-cudnn-cu12        9.1.0.70\r\n",
      "nvidia-cufft-cu12        11.2.1.3\r\n",
      "nvidia-curand-cu12       10.3.5.147\r\n",
      "nvidia-cusolver-cu12     11.6.1.9\r\n",
      "nvidia-cusparse-cu12     12.3.1.170\r\n",
      "nvidia-cusparselt-cu12   0.6.2\r\n",
      "nvidia-nccl-cu12         2.21.5\r\n",
      "nvidia-nvjitlink-cu12    12.4.127\r\n",
      "nvidia-nvtx-cu12         12.4.127\r\n",
      "opencv-python            4.10.0.84\r\n",
      "optree                   0.15.0\r\n",
      "packaging                25.0\r\n",
      "pandas                   2.2.3\r\n",
      "pandas-profiling         3.6.6\r\n",
      "parso                    0.8.4\r\n",
      "patsy                    1.0.1\r\n",
      "pexpect                  4.9.0\r\n",
      "phik                     0.12.4\r\n",
      "pillow                   11.2.1\r\n",
      "pip                      24.2\r\n",
      "platformdirs             4.3.7\r\n",
      "prompt_toolkit           3.0.51\r\n",
      "propcache                0.3.1\r\n",
      "psutil                   7.0.0\r\n",
      "ptyprocess               0.7.0\r\n",
      "pure_eval                0.2.3\r\n",
      "puremagic                1.28\r\n",
      "py-cpuinfo               9.0.0\r\n",
      "pyarrow                  19.0.1\r\n",
      "pydantic                 2.10.6\r\n",
      "pydantic_core            2.27.2\r\n",
      "Pygments                 2.19.1\r\n",
      "pyparsing                3.2.3\r\n",
      "PySocks                  1.7.1\r\n",
      "python-dateutil          2.9.0.post0\r\n",
      "pytz                     2025.2\r\n",
      "PyWavelets               1.8.0\r\n",
      "PyYAML                   6.0.2\r\n",
      "pyzmq                    26.4.0\r\n",
      "regex                    2024.11.6\r\n",
      "requests                 2.32.3\r\n",
      "rich                     14.0.0\r\n",
      "safetensors              0.5.3\r\n",
      "scikit-learn             1.6.1\r\n",
      "scipy                    1.15.2\r\n",
      "seaborn                  0.13.2\r\n",
      "setuptools               75.8.0\r\n",
      "six                      1.17.0\r\n",
      "soupsieve                2.7\r\n",
      "stack-data               0.6.3\r\n",
      "statsmodels              0.14.4\r\n",
      "sympy                    1.13.1\r\n",
      "threadpoolctl            3.6.0\r\n",
      "tokenizers               0.21.1\r\n",
      "torch                    2.6.0\r\n",
      "torchvision              0.21.0\r\n",
      "tornado                  6.4.2\r\n",
      "tqdm                     4.67.1\r\n",
      "traitlets                5.14.3\r\n",
      "transformers             4.51.3\r\n",
      "triton                   3.2.0\r\n",
      "ttach                    0.0.3\r\n",
      "typeguard                4.4.2\r\n",
      "typing_extensions        4.12.2\r\n",
      "tzdata                   2025.2\r\n",
      "ultralytics              8.3.80\r\n",
      "ultralytics-thop         2.0.14\r\n",
      "urllib3                  2.4.0\r\n",
      "visions                  0.8.1\r\n",
      "wcwidth                  0.2.13\r\n",
      "wheel                    0.44.0\r\n",
      "widgetsnbextension       4.0.13\r\n",
      "wordcloud                1.9.4\r\n",
      "xxhash                   3.5.0\r\n",
      "yarl                     1.18.3\r\n",
      "ydata-profiling          4.16.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e56d6",
   "metadata": {},
   "source": [
    "### Generate Synthetic Hazy Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e197c111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation set\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 458 out of 458 | elapsed:    5.1s finished\n"
     ]
    }
   ],
   "source": [
    "!python ./dotah_generator/dotah_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbc387",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c154c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hice1/wchia7/scratch/conda_venvs/DL_project/lib/python310.zip', '/home/hice1/wchia7/scratch/conda_venvs/DL_project/lib/python3.10', '/home/hice1/wchia7/scratch/conda_venvs/DL_project/lib/python3.10/lib-dynload', '', '/home/hice1/wchia7/.local/lib/python3.10/site-packages', '/home/hice1/wchia7/scratch/conda_venvs/DL_project/lib/python3.10/site-packages', '/storage/ice1/6/5/wchia7/DL_group_project']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to sys.path\n",
    "project_root = os.path.abspath(\".\")   # or adjust if needed\n",
    "sys.path.append(project_root)\n",
    "    \n",
    "os.chdir(\"/storage/ice1/6/5/wchia7/DL_group_project/trainers\")\n",
    "os.chdir(\"/storage/ice1/6/5/wchia7/DL_group_project\")\n",
    "os.environ[\"PYTHONPATH\"] = project_root\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d636e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGELOG.md  dotah_generator\t    models\t\t      trainers\r\n",
      "cmd.lnk       etc\t\t    output_models\t      train.py\r\n",
      "config.py     generate_data.bat     __pycache__\t\t      train_rcan.bat\r\n",
      "configs       Instructions.docx     raw_data\t\t      utils\r\n",
      "data\t      KJRD_net_train.ipynb  ReadMe.md\r\n",
      "data_source   mae\t\t    requirements_windows.yml\r\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c25adc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA (gpu: NVIDIA H200).\n",
      "_IncompatibleKeys(missing_keys=['mask_token', 'decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'], unexpected_keys=[])\n",
      "Training KJRD-Net\n",
      "Training batch: 0\n",
      "/storage/ice1/6/5/wchia7/DL_group_project/models/masked_autoencoder.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(img)\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [1/30], Loss: 640.3603\n",
      "Epoch [1] Validation Loss: {'loss_classifier': tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0524, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(390.4692, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [2/30], Loss: 600.0307\n",
      "Epoch [2] Validation Loss: {'loss_classifier': tensor(1.4049, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0067, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0103, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(496.7617, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [3/30], Loss: 575.7712\n",
      "Epoch [3] Validation Loss: {'loss_classifier': tensor(1.3514, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0076, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0311, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(160.1023, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [4/30], Loss: 573.1971\n",
      "Epoch [4] Validation Loss: {'loss_classifier': tensor(0.7757, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0017, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0546, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(110.1357, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [5/30], Loss: 575.9938\n",
      "Epoch [5] Validation Loss: {'loss_classifier': tensor(1.4797, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0631, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(379.4863, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [6/30], Loss: 572.2191\n",
      "Epoch [6] Validation Loss: {'loss_classifier': tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0019, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0465, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(419.4088, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [7/30], Loss: 581.5196\n",
      "Epoch [7] Validation Loss: {'loss_classifier': tensor(1.4505, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0013, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0462, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1127.7468, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [8/30], Loss: 571.7981\n",
      "Epoch [8] Validation Loss: {'loss_classifier': tensor(1.6394, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0304, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0369, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(253.3369, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [9/30], Loss: 582.5639\n",
      "Epoch [9] Validation Loss: {'loss_classifier': tensor(1.5495, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0233, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1348.1532, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [10/30], Loss: 572.8808\n",
      "Epoch [10] Validation Loss: {'loss_classifier': tensor(1.5253, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0518, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(150.9313, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [11/30], Loss: 578.7089\n",
      "Epoch [11] Validation Loss: {'loss_classifier': tensor(1.5775, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0153, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1043, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(734.5868, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [12/30], Loss: 572.2108\n",
      "Epoch [12] Validation Loss: {'loss_classifier': tensor(1.3500, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0022, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0273, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(109.0840, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [13/30], Loss: 572.7491\n",
      "Epoch [13] Validation Loss: {'loss_classifier': tensor(1.4699, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0184, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0589, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(163.6242, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [14/30], Loss: 576.6329\n",
      "Epoch [14] Validation Loss: {'loss_classifier': tensor(1.9311, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0024, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0310, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(636.4596, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Target: tensor([], device='cuda:0', size=(0, 4), dtype=torch.int64)\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [15/30], Loss: 575.3768\n",
      "Epoch [15] Validation Loss: {'loss_classifier': tensor(1.8319, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0048, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0622, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(427.7286, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [16/30], Loss: 572.4699\n",
      "Epoch [16] Validation Loss: {'loss_classifier': tensor(1.5446, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0164, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0478, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(198.3477, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [17/30], Loss: 574.1299\n",
      "Epoch [17] Validation Loss: {'loss_classifier': tensor(1.6042, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0527, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(411.1165, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [18/30], Loss: 577.9001\n",
      "Epoch [18] Validation Loss: {'loss_classifier': tensor(1.7818, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0358, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(996.6738, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [19/30], Loss: 580.6360\n",
      "Epoch [19] Validation Loss: {'loss_classifier': tensor(1.5364, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0725, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0179, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1148.1029, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [20/30], Loss: 578.1366\n",
      "Epoch [20] Validation Loss: {'loss_classifier': tensor(2.3771, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0221, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1060.6653, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [21/30], Loss: 584.3590\n",
      "Epoch [21] Validation Loss: {'loss_classifier': tensor(3.0437, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0042, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0341, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1455.8906, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [22/30], Loss: 588.6365\n",
      "Epoch [22] Validation Loss: {'loss_classifier': tensor(2.1606, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(6.0156e-05, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0361, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1960.9646, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [23/30], Loss: 573.8556\n",
      "Epoch [23] Validation Loss: {'loss_classifier': tensor(2.4874, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0048, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0530, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(264.9965, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [24/30], Loss: 580.2564\n",
      "Epoch [24] Validation Loss: {'loss_classifier': tensor(2.2310, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0011, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0160, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1092.4266, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [25/30], Loss: 569.9935\n",
      "Epoch [25] Validation Loss: {'loss_classifier': tensor(2.3739, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0548, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(191.2989, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [26/30], Loss: 574.5333\n",
      "Epoch [26] Validation Loss: {'loss_classifier': tensor(1.9838, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0108, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0467, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(814.6792, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [27/30], Loss: 569.3938\n",
      "Epoch [27] Validation Loss: {'loss_classifier': tensor(1.7403, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0507, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(153.4663, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [28/30], Loss: 569.4880\n",
      "Epoch [28] Validation Loss: {'loss_classifier': tensor(2.5548, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0488, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(310.5241, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [29/30], Loss: 575.9156\n",
      "Epoch [29] Validation Loss: {'loss_classifier': tensor(2.1680, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0681, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(869.2473, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n",
      "Training batch: 0\n",
      "Training batch: 1\n",
      "Training batch: 2\n",
      "Training batch: 3\n",
      "Training batch: 4\n",
      "Training batch: 5\n",
      "Training batch: 6\n",
      "Training batch: 7\n",
      "Training batch: 8\n",
      "Training batch: 9\n",
      "Training batch: 10\n",
      "Training batch: 11\n",
      "Training batch: 12\n",
      "Training batch: 13\n",
      "Training batch: 14\n",
      "Training batch: 15\n",
      "Training batch: 16\n",
      "Training batch: 17\n",
      "Training batch: 18\n",
      "Training batch: 19\n",
      "Training batch: 20\n",
      "Training batch: 21\n",
      "Training batch: 22\n",
      "Training batch: 23\n",
      "Training batch: 24\n",
      "Training batch: 25\n",
      "Training batch: 26\n",
      "Training batch: 27\n",
      "Training batch: 28\n",
      "Training batch: 29\n",
      "Training batch: 30\n",
      "Training batch: 31\n",
      "Training batch: 32\n",
      "Training batch: 33\n",
      "Training batch: 34\n",
      "Training batch: 35\n",
      "Training batch: 36\n",
      "Training batch: 37\n",
      "Training batch: 38\n",
      "Training batch: 39\n",
      "Training batch: 40\n",
      "Training batch: 41\n",
      "Training batch: 42\n",
      "Training batch: 43\n",
      "Training batch: 44\n",
      "Training batch: 45\n",
      "Training batch: 46\n",
      "Training batch: 47\n",
      "Training batch: 48\n",
      "Training batch: 49\n",
      "Training batch: 50\n",
      "Training batch: 51\n",
      "Training batch: 52\n",
      "Training batch: 53\n",
      "Training batch: 54\n",
      "Training batch: 55\n",
      "Training batch: 56\n",
      "Training batch: 57\n",
      "Training batch: 58\n",
      "Training batch: 59\n",
      "Training batch: 60\n",
      "Training batch: 61\n",
      "Training batch: 62\n",
      "Training batch: 63\n",
      "Training batch: 64\n",
      "Training batch: 65\n",
      "Training batch: 66\n",
      "Training batch: 67\n",
      "Training batch: 68\n",
      "Training batch: 69\n",
      "Training batch: 70\n",
      "Training batch: 71\n",
      "Training batch: 72\n",
      "Training batch: 73\n",
      "Training batch: 74\n",
      "Training batch: 75\n",
      "Training batch: 76\n",
      "Training batch: 77\n",
      "Training batch: 78\n",
      "Training batch: 79\n",
      "Training batch: 80\n",
      "Training batch: 81\n",
      "Training batch: 82\n",
      "Training batch: 83\n",
      "Training batch: 84\n",
      "Training batch: 85\n",
      "Training batch: 86\n",
      "Training batch: 87\n",
      "Training batch: 88\n",
      "Epoch [30/30], Loss: 579.0991\n",
      "Epoch [30] Validation Loss: {'loss_classifier': tensor(3.0887, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0174, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1045.4052, device='cuda:0', grad_fn=<DivBackward0>)}, , Val_loader_length 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./output_models//KJRDnet_main_block_dataset_kjrd.pth\n",
      "Model saved to ./output_models//KJRDnet_detector_dataset_kjrd.pth\n",
      "Completed in 2465.473.\n"
     ]
    }
   ],
   "source": [
    "!python train.py --config_file configs/config_kjrd_net.yaml --output_dir ./output_models/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL_project]",
   "language": "python",
   "name": "conda-env-DL_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
